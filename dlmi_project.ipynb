{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DLMI PROJECT : Low-Dose CT Image Denoising"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors :\n",
    "- JOEL MBOUWE\n",
    "- ADNAN ZEDDOUN \n",
    "- ABDER MEHDAOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTsBLIDPHUVq"
   },
   "source": [
    "# Donn√©es et packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "6WQv2_JrnRBW",
    "outputId": "0f4747bb-0365-450b-bf66-4612a912e543"
   },
   "outputs": [],
   "source": [
    "# %%shell\n",
    "# mkdir imgs && cd imgs\n",
    "# wget https://zenodo.org/record/3384092/files/ground_truth_validation.zip\n",
    "# unzip -q ground_truth_validation.zip\n",
    "# rm ground_truth_validation.zip\n",
    "# wget https://zenodo.org/record/3384092/files/ground_truth_test.zip\n",
    "# unzip -q ground_truth_test.zip\n",
    "# rm ground_truth_test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4r0FGw-0JkV"
   },
   "source": [
    "# Pour le projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAefvqFEHf79",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# from utils import Logger\n",
    "# Pytorch Libraries\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# General Libraries\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "#import zipfile   # no\n",
    "import h5py\n",
    "#import time      # no\n",
    "import torchvision.models as models\n",
    "# Modelisation Libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob    # only glob2\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "from IPython import display\n",
    "# from utils import Logger\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import autograd\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim = 64\n",
    "out_shape = 8\n",
    "BASE_DIR = os.path.abspath('')\n",
    "dataset_LDCT = os.path.abspath(os.path.join(BASE_DIR,  \"..\", \"LoDoPaB-CT 20set\"))\n",
    "print(dataset_LDCT)\n",
    "print(BASE_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxiJJlPe1FsC",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "name_list1 = ['ground_truth_validation/ground_truth_validation_00' + str(i)+ '.hdf5' for i in range(10)]\n",
    "name_list2 = ['ground_truth_validation/ground_truth_validation_0' + str(i)+ '.hdf5' for i in range(10,27)]\n",
    "name_list = name_list1 + name_list2\n",
    "X_train = [h5py.File(i, 'r')['data'] for i in name_list]\n",
    "name_list1_test = ['ground_truth_test/ground_truth_test_00' + str(i)+ '.hdf5' for i in range(10)]\n",
    "name_list2_test = ['ground_truth_test/ground_truth_test_0' + str(i)+ '.hdf5' for i in range(10,27)]\n",
    "name_list_test = name_list1_test + name_list2_test\n",
    "X_test = [h5py.File(i, 'r')['data'] for i in name_list_test]\n",
    "X_train = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tUZuq3P41Fut",
    "outputId": "0a7059a1-eecb-4b9e-f6e1-5450733e8bd4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_list =  []\n",
    "for k in range(49) :\n",
    "  for i in range(128) :\n",
    "    img_list.append(X_train[k][i])\n",
    "val_list =  []\n",
    "for k in range(49, 54) :\n",
    "  for i in range(128) :\n",
    "    val_list.append(X_train[k][i])\n",
    "print(len(val_list))\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0][26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nxsz9FH41Fxe",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class NoisyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_list, data_augmentation_factor, mu, var, transforms=True, only_noise=False, name=\"train\"):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            - img_list: list of the images paths\n",
    "            - data_augmentation_factor: int - number of times an image is augmented. Set\n",
    "            it to 0 if no augmentation is wanted.\n",
    "            - mu: mean of the training set - used for standardization\n",
    "            - var: standard deviation of the training set - used for standardization\n",
    "            - transforms: bool - indicates whether to apply data augmentation \n",
    "            - name: string - name of the dataset (train, validation or test)\n",
    "            - only_noise: bool - whether or not to do residual learning\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "                \n",
    "        if not data_augmentation_factor: self.data_augmentation_factor = 1\n",
    "        else: self.data_augmentation_factor = data_augmentation_factor\n",
    "        self.img_list = img_list\n",
    "        self.transforms = transforms\n",
    "        self.only_noise = only_noise\n",
    "        self.name = name        \n",
    "        self.mean = mu\n",
    "        self.std = var\n",
    "\n",
    "        # Create dictionary that maps each image to some specific noise and the\n",
    "        # image's path\n",
    "        dicts = []\n",
    "        for image_path in img_list:\n",
    "            for _ in range(self.data_augmentation_factor):\n",
    "\n",
    "                # Define noise to be applied\n",
    "                p = np.random.rand()\n",
    "                if p < 1: noise_type = \"gaussian\"\n",
    "                else: noise_type =\"speckle\"\n",
    "\n",
    "                # Add image-noise pair information to the info dictionary\n",
    "                dicts.append({'path': image_path,\n",
    "                              'noise': noise_type})            \n",
    "        self.img_dict = {image_id: info for image_id, info in enumerate(dicts)}\n",
    "\n",
    "\n",
    "    def __getitem__(self, image_id):\n",
    "        np.random.seed(0)\n",
    "\n",
    "        #load image\n",
    "        #img = imageio.imread(self.img_dict[image_id]['path']).astype(np.uint8)\n",
    "        img = self.img_dict[image_id]['path']\n",
    "        #standardize it\n",
    "#         img = (img - img.min()) /(img.max() - img.min())\n",
    "        #downsample the images' size (to speed up training)\n",
    "        img = resize(img, (dim, dim))\n",
    "\n",
    "        #create noisy image \n",
    "        noise_type = self.img_dict[image_id]['noise']       \n",
    "        if noise_type == \"gaussian\":\n",
    "            noise = np.random.normal(0, 0.015, img.shape)\n",
    "        elif noise_type == \"speckle\":  \n",
    "            noise =  img * np.random.randn(img.shape[0], img.shape[1]) / 5            \n",
    "        noisy_img = img + noise\n",
    "        noisy_img = (noisy_img - noisy_img.min()) /(noisy_img.max() - noisy_img.min())\n",
    "        #if residual learning, ground-truth should be the noise\n",
    "        if self.only_noise: img = noise\n",
    "\n",
    "        #convert to PIL images\n",
    "        img = Image.fromarray(img)\n",
    "        noisy_img = Image.fromarray(noisy_img)\n",
    "\n",
    "        #apply the same data augmentation transformations to the input and the \n",
    "        #ground-truth\n",
    "        p = np.random.rand()\n",
    "        if self.transforms and p<0.5: \n",
    "            self.t = T.Compose([T.RandomHorizontalFlip(1), T.ToTensor()])\n",
    "        else: self.t = T.Compose([T.ToTensor()])\n",
    "        img = self.t(img)\n",
    "        noisy_img = self.t(noisy_img)\n",
    "        return noisy_img, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x, y = train[110]\n",
    "x1, y1 = train[150]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "axes[0].imshow(x[0], cmap=\"gray\")\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Noisy Image', fontsize=20)\n",
    "# plt.subplot(122)\n",
    "axes[1].imshow(y[0], cmap=\"gray\")\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Ground truth Image', fontsize=20)\n",
    "# plt.subplot(221)\n",
    "axes[2].imshow(x1[0], cmap=\"gray\")\n",
    "axes[2].axis('off')\n",
    "# plt.subplot(222)\n",
    "axes[3].imshow(y1[0], cmap=\"gray\")\n",
    "axes[3].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "in3yVg9YzMmD",
    "outputId": "d791b4ba-67d8-49f1-acbb-1514fee3d8d5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig('sample', dpi = 100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxVdmofi2Rzb",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    \"\"\" FILL HERE \"\"\"\n",
    "    self.convlayers = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.convlayers(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVRzS3mSEwDi",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=64, kernel_size=3, \n",
    "                stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=64, kernel_size=3,\n",
    "                stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128, kernel_size=3,\n",
    "                stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=128, kernel_size=3,\n",
    "                stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=256, kernel_size=3,\n",
    "                stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256, out_channels=256, kernel_size=3,\n",
    "                stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )     \n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            torch.nn.Linear(256 * out_shape**2, 1024),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )           \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.shape)\n",
    "        # Flatten and apply sigmoid\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 256 * out_shape**2)\n",
    "        x = self.linear(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hynm0qLI5GK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = models.vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x*255.0\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        x[:, 0,:,:]-=103.939\n",
    "        x[:, 1,:,:]-=116.779\n",
    "        x[:, 2,:,:]-=123.68\n",
    "        out = self.feature_extractor(x)\n",
    "        return out\n",
    "features_extractor = FeatureExtractor()\n",
    "for param in features_extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3n-wkBIx3lf",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Lambda = 10\n",
    "batch_size = 32\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rofQt9IQ4bMV",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "only_noise = False\n",
    "train = NoisyDataset(img_list, 1, 0, 1, False, only_noise, name=\"train\")\n",
    "val = NoisyDataset(val_list, 1, 0, 1, False, only_noise, name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmAy2fBV4jr-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val, batch_size=2, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9AJybAN2nxP",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    features_extractor.cuda()\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-5, betas=(0.5, 0.9))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-5, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6im8TxFoK21M",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def vgg_loss(pred, gt, features_extractor = features_extractor):\n",
    "  #vgg_pred = features_extractor(pred)\n",
    "  vgg_gt = features_extractor(gt) \n",
    "  vgg_pred = features_extractor(pred)\n",
    "  size = vgg_gt.shape\n",
    "#   normalized = 1/(size[1]*size[2]*size[3])\n",
    "  mse = nn.MSELoss()\n",
    "  return mse(vgg_pred, vgg_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWqr-9zwEvjP",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(discriminator, real_data, fake_data, Lambda, batch_size=16):\n",
    "\n",
    "    alpha = torch.rand(real_data.shape[0], 1).cuda()\n",
    "    alpha = alpha.expand(real_data.shape[0], int(real_data.nelement()/real_data.shape[0])).contiguous()\n",
    "    alpha = alpha.view(real_data.shape[0], 1, dim, dim)\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * Lambda\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_discriminator_generator(optimizer_d, optimizer_g, real_data, input_img,\n",
    "                                  batch_size, loss_function, Lambda=10, lambda_2=10):\n",
    "\n",
    "   # real_data = Normal DCT & input_img = Low DTC\n",
    "    #1. Train Discriminator\n",
    "    #Reset gradients\n",
    "    fake_data = generator(input_img)\n",
    "    for k in range(2) :\n",
    "      optimizer_d.zero_grad()\n",
    "      #1.1 Train on Real Data\n",
    "      prediction_real = discriminator(real_data)\n",
    "      # Calculate error and backpropagate\n",
    "      error_real = -torch.mean(prediction_real)\n",
    "\n",
    "      # 1.2 Train on Fake Data\n",
    "      prediction_fake = discriminator(fake_data.detach())\n",
    "      # Calculate error and backpropagate\n",
    "      error_fake = torch.mean(prediction_fake)\n",
    "      gradient_penalty = calc_gradient_penalty(discriminator, real_data, \n",
    "                                              fake_data.detach(), Lambda=Lambda,\n",
    "                                              batch_size=batch_size)\n",
    "\n",
    "      loss_discriminator = error_real + error_fake\n",
    "      loss_discriminator.backward()\n",
    "      gradient_penalty.backward()\n",
    "\n",
    "      # 1.3 Update weights with gradients\n",
    "      optimizer_d.step()\n",
    "\n",
    "    # 2. Train Generator\n",
    "    # Reset gradients\n",
    "    optimizer_g.zero_grad()\n",
    "    # generate fake data\n",
    "#     fake_data = generator(input_img)\n",
    "    prediction_img = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    loss_generator = -torch.mean(prediction_img) \n",
    "    loss_vgg = lambda_2 * loss_function(fake_data, real_data)\n",
    "    loss_generator_t = loss_generator + loss_vgg \n",
    "    loss_generator_t.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer_g.step()   \n",
    "\n",
    "    # print(loss_discriminator, loss_generator, loss_vgg, gradient_penalty)\n",
    "    return loss_discriminator, loss_generator, loss_vgg, gradient_penalty, error_real.mean(), error_fake.mean(), fake_data\n",
    "def validate_discriminator_generator(real_data, input_img,\n",
    "                                    batch_size, loss_function,\n",
    "                                     Lambda=10, lambda_2=10):\n",
    "\n",
    "   # real_data = Normal DCT & input_img = Low DTC\n",
    "    fake_data = generator(input_img)\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = -torch.mean(prediction_real)\n",
    "    prediction_fake = discriminator(fake_data.detach())\n",
    "    error_fake = torch.mean(prediction_fake)\n",
    "    gradient_penalty = calc_gradient_penalty(discriminator, real_data, \n",
    "                                              fake_data.detach(), Lambda=Lambda,\n",
    "                                              batch_size=batch_size)\n",
    "\n",
    "    loss_discriminator = error_real + error_fake\n",
    "    loss_generator = - error_fake\n",
    "    loss_vgg = lambda_2 * loss_function(fake_data, real_data)\n",
    "\n",
    "    return loss_discriminator, loss_generator, loss_vgg, gradient_penalty, error_real.mean(), error_fake.mean(), fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr0K7BZg51uK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for n_batch, (test_images, real_test_img) in enumerate(val_data_loader) :\n",
    "    if n_batch ==1:\n",
    "        break\n",
    "input_test_img = test_images.cuda()\n",
    "real_test_img = real_test_img.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDkjN2N4wb1D",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_prediction(inputs, outputs, ground_truth, only_noise, plot=False, epoch='0'):\n",
    "    size = inputs.shape\n",
    "    inputs = np.vstack(inputs.data.cpu().numpy().reshape(size[0], size[2], size[3]))\n",
    "    outputs = np.vstack(outputs.data.cpu().numpy().reshape(size[0], size[2], size[3]))\n",
    "    ground_truth = np.vstack(ground_truth.data.cpu().numpy().reshape(size[0], size[2], size[3]))\n",
    "    for i in range(1):\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        ax1 = fig.add_subplot(1, 3, 1)\n",
    "        ax2 = fig.add_subplot(1, 3, 2)\n",
    "        ax3 = fig.add_subplot(1, 3, 3)\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax3.axis('off')\n",
    "        ax1.set_title(\"Noisy image\")\n",
    "        ax2.set_title(\"Denoised image\")\n",
    "        ax3.set_title(\"Ground-truth\")       \n",
    "\n",
    "        if only_noise:\n",
    "            ax1.imshow(inputs, cmap='gray')\n",
    "            ax2.imshow(inputs - outputs[0], cmap='gray')\n",
    "            ax3.imshow(inputs - ground_truth[0], cmap='gray')\n",
    "        else:\n",
    "            ax1.imshow(inputs, cmap='gray')\n",
    "            ax2.imshow(outputs, cmap='gray')\n",
    "            ax3.imshow(ground_truth, cmap='gray')\n",
    "        if plot: plt.show()\n",
    "        fig.savefig(\"images/epoch_\" + epoch, dpi = 200)\n",
    "\n",
    "def display_status(epoch, num_epochs, n_batch, \n",
    "                    p_real, loss_vgg, p_fake, penalty,\n",
    "                    batch_ = int(len(data_loader.dataset)/batch_size)) :\n",
    "  print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "  epoch, num_epochs, n_batch, batch_))\n",
    "  print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "  print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(p_real, p_fake))\n",
    "  print('Loss VGG : {:.4f}'.format(loss_vgg.data.cpu()))\n",
    "  print('Gradient penalty : {:.4f}'.format(penalty.data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R9DiWBJZGRRS",
    "outputId": "5f64635f-e098-4240-f123-eae921145279",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def MSE(image_true, image_generated):\n",
    "    return ((image_true - image_generated) ** 2).mean()\n",
    "\n",
    "\n",
    "def PSNR(image_true, image_generated):\n",
    "    mse = MSE(image_true, image_generated)\n",
    "    return -10 * torch.log10(mse)\n",
    "\n",
    "\n",
    "def SSIM(image_true, image_generated, C1=0.01, C2=0.03):\n",
    "    mean_true = image_true.mean()\n",
    "    mean_generated = image_generated.mean()\n",
    "    std_true = image_true.std()\n",
    "    std_generated = image_generated.std()\n",
    "    covariance = ((image_true - mean_true) * (image_generated - mean_generated)).mean()\n",
    "\n",
    "    numerator = (2 * mean_true * mean_generated + C1) * (2 * covariance + C2)\n",
    "    denominator = (mean_true ** 2 + mean_generated ** 2 + C1) * (std_true ** 2 + std_generated ** 2 + C2)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NauPO6_w3NCH"
   },
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZwoLoNkHBkF"
   },
   "source": [
    "## Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-P588Z08vgq",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loss_function = {'d_error' : {'train' : [], 'test' : []},\n",
    "                 'g_error' : {'train' : [], 'test' : []},\n",
    "                 'loss_vgg' : {'train' : [], 'test' : []},\n",
    "                 'penalty' : {'train' : [], 'test' : []},\n",
    "                 'p_real' : {'train' : [], 'test' : []},\n",
    "                 'p_fake' : {'train' : [], 'test' : []},\n",
    "                'PSNR' : {'train' : [], 'test' : []},\n",
    "                'SSIM' : {'train' : [], 'test' : []}}\n",
    "def update_loss_function(d_error, g_error, loss_vgg, \n",
    "                         penalty , p_real, p_fake, psnr, ssim, mode) :\n",
    "    loss_function['d_error'][mode].append(d_error.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['g_error'][mode].append(g_error.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['loss_vgg'][mode].append(loss_vgg.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['penalty'][mode].append(penalty.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['p_real'][mode].append(-p_real.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['p_fake'][mode].append(p_fake.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['PSNR'][mode].append(psnr.data.cpu().numpy().reshape(1)[0])\n",
    "    loss_function['SSIM'][mode].append(ssim.data.cpu().numpy().reshape(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "name = '_wgan_vgg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "16yf3QNdyX1a",
    "outputId": "46b060d7-c23b-4c9e-e8d7-e20cd57630f0",
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (input_img, real_data) in enumerate(data_loader):\n",
    "        real_data = Variable(real_data)\n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        if torch.cuda.is_available(): input_img = input_img.cuda()\n",
    "        # Train D & G\n",
    "        d_error, g_error, loss_vgg, penalty , p_real, p_fake, pred_images = train_discriminator_generator(d_optimizer, \n",
    "                                                         g_optimizer,\n",
    "                                                         real_data,\n",
    "                                                         input_img,\n",
    "                                                         loss_function=vgg_loss,\n",
    "                                                         batch_size=batch_size)\n",
    "        \n",
    "#         pred_images = generator(input_img)\n",
    "#         g_optimizer.zero_grad()\n",
    "#         loss_vgg = nn.MSELoss()(pred_images, real_data); loss_vgg.backward()\n",
    "#         g_optimizer.step()\n",
    "        # Display Progress\n",
    "        if (n_batch) % 120  == 0:\n",
    "            display.clear_output(False)\n",
    "            # Display Images\n",
    "            psnr = PSNR(real_data, pred_images)\n",
    "            ssim = SSIM(real_data, pred_images)\n",
    "            update_loss_function(d_error, g_error, loss_vgg, \n",
    "                         penalty , p_real, p_fake, psnr, ssim, mode='train')\n",
    "            d_error, g_error, loss_vgg_test, penalty , p_real, p_fake, pred_images = validate_discriminator_generator(real_test_img, input_test_img,\n",
    "                                                                          batch_size=2, loss_function=vgg_loss)\n",
    "#             pred_images = generator(input_test_img)\n",
    "#             loss_vgg_test = nn.MSELoss()(real_test_img, pred_images)\n",
    "            psnr = PSNR(real_test_img, pred_images)\n",
    "            ssim = SSIM(real_test_img, pred_images)\n",
    "            update_loss_function(d_error, g_error, loss_vgg_test, \n",
    "                         penalty, p_real, p_fake, psnr, ssim, mode='test')\n",
    "\n",
    "#             pred_images = generator(input_test_img.cuda()).data.cpu()\n",
    "            display_status(epoch, num_epochs + 10, n_batch, \n",
    "                    p_real, loss_vgg, p_fake, penalty, \n",
    "                    batch_ = int(len(data_loader.dataset)/batch_size)) \n",
    "            if epoch < 100 :\n",
    "              epoch_ = '0' + str(epoch)\n",
    "            else :\n",
    "                epoch_ = epoch\n",
    "            plot_prediction(input_test_img, pred_images, real_test_img, \n",
    "                            only_noise, plot=True, \n",
    "                            epoch= str(epoch_) + \"_\" + str(n_batch))\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "colab_type": "code",
    "id": "fS3_lc_6Ko-b",
    "outputId": "174921ca-7794-4f69-a71f-fcfb12f98c0a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize =(30, 20))\n",
    "ax = ax.flatten()\n",
    "i = 0\n",
    "for key, values in loss_function.items() :\n",
    "#     if key in ['SSIM', 'loss_vgg', 'PSNR']:\n",
    "    ax[i].plot(values['test'][2:], label='test')\n",
    "    ax[i].plot(values['train'][2:], label='train')\n",
    "    ax[i].set_title(key)\n",
    "    ax[i].legend(loc='best')\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyGMCZBZpHyj",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'generator'+ name)\n",
    "torch.save(discriminator.state_dict(), 'discriminator' + name)\n",
    "np.save(name, loss_function) \n",
    "fig.savefig('loss_gen' + name +'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "colab_type": "code",
    "id": "d1at56wXJnOk",
    "outputId": "77cd0b45-5d62-4c8a-ad4b-f1ac5e635537"
   },
   "source": [
    "# Evaluation des diff√©rents mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "path_models = ['generator_alone_mse', 'generator_alone_vgg',        \n",
    "               'generator_wgan_mse', 'generator_wgan_vgg']\n",
    "dico_metrics = {elt : {'ssim' : [], 'psnr' : []} for elt in path_models}\n",
    "\n",
    "def evaluation(path, data_loader, discriminator = None) :\n",
    "    generator.load_state_dict(torch.load('olds_models/'  + path))\n",
    "    generator.eval()\n",
    "    print(\"Testing model :\", path)\n",
    "    for n_batch, (input_img, real_data) in enumerate(data_loader):\n",
    "        real_data = Variable(real_data)\n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        if torch.cuda.is_available(): input_img = input_img.cuda()\n",
    "        pred_images = generator(input_img)\n",
    "        psnr = PSNR(real_data, pred_images).data.cpu().numpy().reshape(1)[0]\n",
    "        ssim = SSIM(real_data, pred_images).data.cpu().numpy().reshape(1)[0]\n",
    "        dico_metrics[path]['ssim'].append(ssim)\n",
    "        dico_metrics[path]['psnr'].append(psnr)\n",
    "    #plot_prediction(input_img[:3], pred_images[:3], real_data[:3], \n",
    "    #            only_noise, plot=True, \n",
    "    #            epoch=path)\n",
    "    return input_img[-1], pred_images[-1], real_data[-1]\n",
    "\n",
    "to_plot = []\n",
    "for path in path_models : \n",
    "    to_plot.append(evaluation(path, val_data_loader, discriminator = None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of the output of our models on a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize = (15, 10))\n",
    "axes[0, 0].imshow(to_plot[0][0].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "axes[0, 1].imshow(to_plot[0][-1].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "axes[0, 2].imshow(to_plot[0][1].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "axes[1, 0].imshow(to_plot[1][1].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "axes[1, 1].imshow(to_plot[2][1].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "axes[1, 2].imshow(to_plot[3][1].data.cpu().numpy().reshape(64, 64), cmap='gray')\n",
    "\n",
    "axes[0, 0].set_title('Noisy image', fontsize=18)\n",
    "axes[0, 1].set_title('Ground truth image', fontsize=18)\n",
    "axes[0, 2].set_title('Generator-MSE', fontsize=18)\n",
    "axes[1, 0].set_title('Generator-VGG', fontsize=18)\n",
    "axes[1, 1].set_title('WGAN-MSE', fontsize=18)\n",
    "axes[1, 2].set_title('WGAN-VGG', fontsize=18)\n",
    "axes = axes.flatten()\n",
    "for i in range (len(axes)) : \n",
    "    axes[i]\n",
    "    axes[i].axis('off')\n",
    "fig.savefig('image_test', dpi = 100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(dico_metrics)\n",
    "for col in df.columns :\n",
    "    df[col] = df[col].apply(lambda x: x[-1])\n",
    "    df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the mean of our metrics on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(dico_metrics)\n",
    "for col in df.columns :\n",
    "    df[col] = df[col].apply(lambda x: np.mean(x))\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loss_gen_alone_mse = np.load('_alone_mse.npy', allow_pickle='TRUE').item()['loss_vgg']['train']\n",
    "loss_gen_alone_vgg = np.load('_alone_vgg.npy', allow_pickle='TRUE').item()['loss_vgg']['train']\n",
    "loss_gen_wgan_mse = np.load('_wgan_mse.npy', allow_pickle='TRUE').item()['loss_vgg']['train']\n",
    "loss_gen_wgan_vgg = np.load('_wgan_vgg.npy', allow_pickle='TRUE').item()['loss_vgg']['train']\n",
    "loss_disc_wgan_mse = np.load('_wgan_mse.npy', allow_pickle='TRUE').item()\n",
    "loss_disc_wgan_vgg = np.load('_wgan_vgg.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(gs[0, 0]) # row 0, col 0\n",
    "ax1.plot(loss_gen_alone_mse[:120], label = 'Generator-MSE : MSE-Loss')\n",
    "ax1.plot(10*np.array(loss_gen_wgan_mse)[:120], label = 'WGAN-MSE : MSE-Loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_xlabel('epoch');\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1]) # row 0, col 1\n",
    "ax2.plot(loss_gen_alone_vgg[:120], label = 'Generator-VGG : VGG-Loss')\n",
    "ax2.plot(10*np.array(loss_gen_wgan_vgg)[:120], label = 'WGAN-VGG : VGG-Loss')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_xlabel('epoch');\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 0]) # row 1, span all columns\n",
    "ax3.plot(loss_disc_vgg['p_real']['train'][:130], label = 'WGAN-MSE : D(x)')\n",
    "ax3.plot(loss_disc_wgan['p_real']['train'][:130], label = 'WGAN-VGG : D(x)')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_xlabel('epoch');\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 1]) # row 1, span all columns\n",
    "ax4.plot(loss_disc_wgan_mse['p_fake']['train'][:130], label = 'WGAN-MSE : D(G(x))')\n",
    "ax4.plot(loss_disc_wgan_vgg['p_fake']['train'][:130], label = 'WGAN-VGG : D(G(x))')\n",
    "ax4.legend(loc='best')\n",
    "ax4.set_xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig('all_loss.png', dpi =100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "loss_disc_vgg_al = np.load('_alone_vgg.npy', allow_pickle='TRUE').item()\n",
    "loss_disc_mse_al = np.load('_alone_mse.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize = (15, 10))\n",
    "axes[0].plot(loss_disc_wgan_mse['PSNR']['test'][:130], label = 'WGAN-MSE')\n",
    "axes[0].plot(loss_disc_wgan_vgg['PSNR']['test'][:130], label = 'WGAN-VGG')\n",
    "axes[0].plot(loss_disc_vgg_al['PSNR']['train'][:130], label = 'Generator-VGG')\n",
    "axes[0].plot(loss_disc_mse_al['PSNR']['train'][:130], label = 'Generator-MSE')\n",
    "\n",
    "axes[1].plot(loss_disc_wgan_mse['SSIM']['test'][:130], label = 'WGAN-MSE')\n",
    "axes[1].plot(loss_disc_wgan_vgg['SSIM']['test'][:130], label = 'WGAN-VGG')\n",
    "axes[1].plot(loss_disc_vgg_al['SSIM']['train'][:130], label = 'Generator-VGG')\n",
    "axes[1].plot(loss_disc_mse_al['SSIM']['train'][:130], label = 'Generator-MSE')\n",
    "axes[0].set_title('PSNR over epochs')\n",
    "axes[1].set_title('SSIM over epochs')\n",
    "axes[0].legend(loc='best')\n",
    "axes[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig('metrics_epochs.png', dpi =100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4ECEbKiz4BGK"
   ],
   "name": "tmp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "Python (myenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# -*- coding: utf-8 -*-"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}